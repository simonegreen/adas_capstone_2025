{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f442002",
   "metadata": {},
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c2e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c02a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7652df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from seaborn) (3.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d895795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\n",
      "  Using cached scikit-learn-extra-0.3.0.tar.gz (818 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from scikit-learn-extra) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from scikit-learn-extra) (1.4.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from scikit-learn-extra) (1.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/s_gre1/miniconda3/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.0)\n",
      "Building wheels for collected packages: scikit-learn-extra\n",
      "  Building wheel for scikit-learn-extra (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-learn-extra: filename=scikit_learn_extra-0.3.0-cp310-cp310-macosx_11_0_arm64.whl size=390035 sha256=a2811e2d97f38c2f962a6e68292fc25f26433d9142111d903fbd4340dd84f74b\n",
      "  Stored in directory: /Users/s_gre1/Library/Caches/pip/wheels/89/a1/b9/758739c49b7f3e0a944e04247341258e15ce13016fbd23628b\n",
      "Successfully built scikit-learn-extra\n",
      "Installing collected packages: scikit-learn-extra\n",
      "Successfully installed scikit-learn-extra-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f00d1a",
   "metadata": {},
   "source": [
    "### Helper Functions: Algorithms & Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5c78b",
   "metadata": {},
   "source": [
    "**Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8fe4e",
   "metadata": {},
   "source": [
    "DONE: add a \"mode\" argument to each algorithm that, if mode = 1 the cluster labelling is output and if mode = 0 the silhouette coefficient is output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c8acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Prevents parallelism issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfd14e3-20b2-4404-a2ea-ff8f7e5aa83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for KMeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\"\"\"\n",
    "Performs KMeans clustering using the data from selected_features.\n",
    "If mode = 0, the silhouette score of the clustering is returned.\n",
    "If mode = 1, the labels of the clustering is returned.\n",
    "\"\"\"\n",
    "def kmeans_clustering(selected_features,mode, n_clusters=2, max_iter=300):\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering on the input samples\n",
    "    \n",
    "    Parameters:\n",
    "        samples: array-like, shape (n_samples, n_features)\n",
    "        n_clusters: int, number of clusters (default=2)\n",
    "        max_iter: int, maximum iterations (default=300)\n",
    "    \n",
    "    Returns:\n",
    "        silhouette_coef: silhouette coefficient score\n",
    "    \"\"\"\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    best_k = n_clusters\n",
    "\n",
    "    try:\n",
    "        k_options = range(2, 6)\n",
    "        best_k = max(k_options, key=lambda k: silhouette_score(X_scaled, KMeans(n_clusters=k, random_state=SEED, init='random', n_init=1).fit_predict(X_scaled)))\n",
    "    except:\n",
    "        best_k = 2\n",
    "    \n",
    "    k_means = KMeans(n_clusters=best_k, max_iter=max_iter, random_state=SEED, init='random', n_init=1)\n",
    "    k_means.fit(X_scaled)\n",
    "    if mode == 0:\n",
    "        try:\n",
    "            silhouette_coef = silhouette_score(X_scaled, k_means.labels_)\n",
    "        except ValueError:\n",
    "            silhouette_coef = -1  # Assigning lowest score if clustering fails\n",
    "        return silhouette_coef, k_means.labels_\n",
    "    if mode == 1:\n",
    "        return k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d63cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM Clustering Code\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Performs EM clustering using the data from selected_features.\n",
    "If mode = 0, the silhouette score of the clustering is returned.\n",
    "If mode = 1, the labels of the clustering is returned.\n",
    "\"\"\"\n",
    "def em_clustering(selected_features, mode, n_clusters=2):\n",
    "    \"\"\"\n",
    "    Perform EM Clustering on selected features and return silhouette score.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Silhouette score of the clustering (-1 if clustering fails)\n",
    "    \"\"\"\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize and fit the EM model\n",
    "    em_model = GaussianMixture(\n",
    "        n_components=n_clusters,\n",
    "        random_state=SEED, #THOUGHTS: We can improve this later to have an array of seeds to select from to observe variations\n",
    "        n_init=1  # forces one initialization to remove variability\n",
    "        \n",
    "    )\n",
    "    \n",
    "   \n",
    "    try:\n",
    "        # Fit the model and get cluster assignments\n",
    "        em_model.fit(X_scaled)\n",
    "        labels = em_model.predict(X_scaled)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except Exception as e:\n",
    "        #print(f\"Clustering failed: {str(e)}\")\n",
    "        silhouette_coef = -1  # Assigning lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef, labels\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9944db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Detection method: \n",
    "# I put 'optimization part' in 'DBSCAN_Optimization_Code.ipynb' file. \n",
    "# We can use optimization after initial run to do a comparison and analysis in our paper to show improvements.\n",
    "\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# DONE: keep -1 --> they will be its own cluster\n",
    "\"\"\"\n",
    "Performs DBSCAN clustering using the data from selected_features.\n",
    "If mode = 0, the silhouette score of the clustering is returned.\n",
    "If mode = 1, the labels of the clustering is returned.\n",
    "\"\"\"\n",
    "def dbscan_clustering(selected_features, mode, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on selected features\n",
    "    \n",
    "    Parameters:\n",
    "    selected_features : pandas DataFrame\n",
    "        The features selected for clustering\n",
    "    eps : float\n",
    "        The maximum distance between two samples for them to be considered neighbors\n",
    "    min_samples : int\n",
    "        The number of samples in a neighborhood for a point to be considered a core point\n",
    "        \n",
    "    Returns:\n",
    "    float : silhouette coefficient\n",
    "    dict : additional clustering information\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Initialize and fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "    if -1 in labels:\n",
    "        labels[labels == -1] = max(labels) +1\n",
    "\n",
    "     # Get number of clusters (including noise points which are labeled -1, K Medoids does not have noise points)\n",
    "    n_clusters = len(set(labels))\n",
    "    \n",
    "    # calculate silhouette score if more than one cluster and  noise points\n",
    "    if n_clusters > 1:\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    else:\n",
    "        silhouette_coef = -1  # Assign lowest score if clustering fails\n",
    "\n",
    "    \n",
    "    # NOTE: -- Uncomment when we analyze and optimize ---- Additional clustering information\n",
    "    # info = {\n",
    "    #     'n_clusters': n_clusters,\n",
    "    #     'n_noise': list(labels).count(-1),\n",
    "    #     'labels': labels,\n",
    "    #     'cluster_sizes': pd.Series(labels).value_counts().to_dict()\n",
    "    # }\n",
    "    \n",
    "    if mode == 0:\n",
    "        return silhouette_coef, labels\n",
    "    if mode == 1:\n",
    "        return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1869dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for K Medoids\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\"\n",
    "Performs KMediods clustering using the data from selected_features.\n",
    "If mode = 0, the silhouette score of the clustering is returned.\n",
    "If mode = 1, the labels of the clustering is returned.\n",
    "\"\"\"\n",
    "def modified_kmedoids_clustering(selected_features, mode, n_clusters=2):\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    best_k = n_clusters\n",
    "    try:\n",
    "        k_options = range(2, 6)\n",
    "        best_k = max(k_options, key=lambda k: silhouette_score(X_scaled, KMedoids(n_clusters=k, method='alternate', init='k-medoids++', max_iter=1500, random_state=SEED).fit_predict(X_scaled)))\n",
    "    except:\n",
    "        best_k = 2\n",
    "\n",
    "     # Initialize and fit the K-Medoids model\n",
    "    kmedoids = KMedoids(n_clusters=best_k, method='alternate', init='k-medoids++', max_iter=1500,random_state=SEED)\n",
    "    \n",
    "\n",
    "    # Calculate silhouette score\n",
    "    try:\n",
    "        labels = kmedoids.fit_predict(X_scaled)\n",
    "        if len(set(labels)) > 1:\n",
    "            silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "        else:\n",
    "            silhouette_coef = 0 # Assigning lowest score if there is only 1 cluster\n",
    "    except Exception as e:\n",
    "        silhouette_coef = -1  # Assigning lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef, labels\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16776843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for K Medoids\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\"\n",
    "Performs KMediods clustering using the data from selected_features.\n",
    "If mode = 0, the silhouette score of the clustering is returned.\n",
    "If mode = 1, the labels of the clustering is returned.\n",
    "\"\"\"\n",
    "def kmedoids_clustering(selected_features, mode, n_clusters=2):\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "     # Initialize and fit the K-Medoids model\n",
    "    kmedoids = KMedoids(n_clusters=n_clusters, method='pam', max_iter=1500, random_state=SEED)\n",
    "    \n",
    "\n",
    "    # Calculate silhouette score\n",
    "    try:\n",
    "        labels = kmedoids.fit_predict(X_scaled)\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except ValueError:\n",
    "        silhouette_coef = -1  # Assigning lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef, labels\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148b46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Mean Shift\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\"\n",
    "Performs Mean Shift clustering using the data from selected_features.\n",
    "If mode = 0, the silhouette score of the clustering is returned.\n",
    "If mode = 1, the labels of the clustering is returned.\n",
    "\"\"\"\n",
    "def modified_meanshift_clustering(selected_features, mode, quantile=0.2, n_samples=500):\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Estimate optimal bandwidth\n",
    "    bandwidth = estimate_bandwidth(X_scaled, quantile=quantile, n_samples=n_samples, random_state=SEED)\n",
    "    if bandwidth <= 0:\n",
    "        bandwidth = 1.0  # Fallback in case of extremely small bandwidth\n",
    "        \n",
    "    # Initialize and fit the Mean Shift model\n",
    "    meanshift = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    # Calculate silhouette score\n",
    "    try:\n",
    "        # print(\"in try\")\n",
    "        labels = meanshift.fit_predict(X_scaled)\n",
    "        n_clusters = len(set(labels))\n",
    "\n",
    "        # if -1 in labels:\n",
    "        #     labels[labels == -1] = n_clusters\n",
    "        # Check the number of clusters determined \n",
    "        #n_clusters = len(np.unique(labels))\n",
    "        #print(f\"Number of clusters found: {n_clusters}\")\n",
    "        if n_clusters > 1:\n",
    "            silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "        else:\n",
    "            silhouette_coef = -1\n",
    "    except Exception as e:\n",
    "        # print(\"in except\")\n",
    "        silhouette_coef = -1 # Assign lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef, labels\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078cfb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for Mean Shift\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\"\n",
    "Performs Mean Shift clustering using the data from selected_features.\n",
    "If mode = 0, the silhouette score of the clustering is returned.\n",
    "If mode = 1, the labels of the clustering is returned.\n",
    "\"\"\"\n",
    "def meanshift_clustering(selected_features, mode, bandwidth=None):\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize and fit the Mean Shift model\n",
    "    meanshift = MeanShift(bandwidth=bandwidth)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    try:\n",
    "        labels = meanshift.fit_predict(X_scaled)\n",
    "        # Check the number of clusters determined \n",
    "        n_clusters = len(np.unique(labels))\n",
    "        #print(f\"Number of clusters found: {n_clusters}\")\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except ValueError:\n",
    "        silhouette_coef = -1  # Assign lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef, labels\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d2115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts the binary value of state (which represents features selected) \n",
    "to a string output res and returns.\n",
    "\"\"\"\n",
    "def bin_to_features(state, mode):\n",
    "  state_bin = bin(state)\n",
    "  #print(state_bin)\n",
    "  state_bin_arr = np.array([b for b in state_bin[2:]])\n",
    "  #pad with zeros\n",
    "  diff = len(FEATURES) - len(state_bin_arr)\n",
    "  padded_arr = np.insert(state_bin_arr, 0, ['0' for i in range(diff)])\n",
    "  (padded_arr)\n",
    "\n",
    "  # identify which indexes are 1\n",
    "  idx = (np.where(padded_arr == '1')[0]).tolist()\n",
    "  #print(idx)\n",
    "  # select feature headings\n",
    "  selected_features = original_features.iloc[:,idx]\n",
    "  features = selected_features.columns.tolist()\n",
    "  res = f\"Features Used: {features}\"\n",
    "  if mode == 0: # return actual feature list\n",
    "    return selected_features\n",
    "  if mode == 1: # return string of feature list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740d094",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cdace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dst_host_count', 'Dst_host_diff_srv_rate', 'Dst_host_serror_rate', 'Dst_host_srv_count', 'Logged_in', 'Protocol_type', 'Same_srv_rate', 'Serror_rate', 'Service', 'Srv_serror_rate']\n"
     ]
    }
   ],
   "source": [
    "# features = np.array(['action', 'availability', 'device_type:1', 'direction',\n",
    "#        'event_type', 'interface_status', 'patch_description',\n",
    "#        'patch_status', 'severity:1', 'traffic_direction'])\n",
    "\n",
    "# Replace feature list input with selected features\n",
    "# features = np.array(['action', 'behavior_name', 'event_id:4', 'exploit_available',\n",
    "#  'file_signature_status', 'network_interface', 'patch_name', 'severity',\n",
    "#  'tcp_flags', 'vulnerability_solution']) # features for generated data complete\n",
    "\n",
    "#features = np.array(['avg_bytes_sent','avg_bytes_received','avg_packets_transferred','avg_flow_duration','recent_tcp_flags','recent_protocol','avg_cpu_usage','avg_memory_usage','avg_disk_usage','avg_uptime']) # features for generated data\n",
    "\n",
    "# IoT23\n",
    "#features = np.array(['conn_state', 'history', 'id.orig_p', 'missed_bytes', 'orig_bytes', 'orig_ip_bytes', 'orig_pkts', 'resp_bytes', 'resp_ip_bytes','resp_pkts'])\n",
    "# NB15\n",
    "#features = np.array(['Djit', 'Dload', 'ct_dst_sport_ltm', 'ct_src_dport_ltm', 'ct_srv_dst', 'ct_state_ttl', 'dmeansz', 'res_bdy_len', 'sloss', 'state'])\n",
    "# KDD'99\n",
    "features = np.array(['Dst_host_count', 'Dst_host_diff_srv_rate', 'Dst_host_same_src_port_rate', 'Dst_host_same_srv_rate', 'Dst_host_srv_count', 'Flag', 'Logged_in', 'Protocol_type', 'Serror_rate', 'Service'])\n",
    "# NSL-KDD\n",
    "#features = np.array(['Diff Srv Rate', 'Dst Host Count', 'Dst Host Diff Srv Rate', 'Dst Host Same Src Port Rate', 'Dst Host Same Srv Rate', 'Flag', 'Logged In', 'Protocol Type', 'Same Srv Rate', 'Service'])\n",
    "\n",
    "\n",
    "features = features.tolist()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Djit         Dload  ct_dst_sport_ltm  ct_src_dport_ltm  ct_srv_dst  \\\n",
      "0     0.292512  1.024719e+06                 1                 1           5   \n",
      "1     0.000000  7.213779e+05                 1                 1           3   \n",
      "2     0.000000  7.170191e+05                 1                 1           4   \n",
      "3     0.325278  1.017857e+06                 1                 1           8   \n",
      "4     0.000000  6.512562e+05                 1                 1           1   \n",
      "5    68.279484  9.480692e+03                 1                 1           1   \n",
      "6    21.213361  6.184004e+04                 1                 1          14   \n",
      "7  7735.661800  6.782899e+04                 1                 1           1   \n",
      "8    74.834585  3.614229e+06                 1                 1          12   \n",
      "9     0.000000  6.780952e+05                 1                 3           3   \n",
      "\n",
      "   ct_state_ttl  dmeansz  res_bdy_len  sloss  state  \n",
      "0             0       76            0      0      0  \n",
      "1             0       89            0      0      0  \n",
      "2             0       89            0      0      0  \n",
      "3             0       76            0      0      0  \n",
      "4             0       81            0      0      0  \n",
      "5             0       53            0      4      1  \n",
      "6             0      103            0      7      1  \n",
      "7             0      565         3924      3      1  \n",
      "8             0      560            0      7      1  \n",
      "9             0       89            0      0      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# FEATURES = {0: 'avg_bytes_sent', 1: 'avg_bytes_received', 2: 'avg_packets_transferred', \n",
    "#   3: 'avg_flow_duration', 4: 'recent_tcp_flags', 5: 'recent_protocol', 6: 'avg_cpu_usage', \n",
    "#   7: 'avg_memory_usage', 8: 'avg_disk_usage', 9: 'avg_uptime'}\n",
    "# features = list(FEATURES.values())\n",
    "FEATURES = {k:str(v) for k,v in zip(range(len(features)), features) }\n",
    "#print(FEATURES)\n",
    "#data = pd.read_csv(\"../data/real-world/data-cleaning/cleaned_NB15_1_sub1.csv\")\n",
    "#data = pd.read_csv(\"../data/real-world/data-cleaning/cleaned_iot23_RW21.csv\")\n",
    "data = pd.read_csv(\"../data/real-world/data-cleaning/cleaned_kdd_sample_1.csv\")\n",
    "#data = pd.read_csv(\"../data/real-world/data-cleaning/cleaned_nsl_kdd_sample_1.csv\")\n",
    "\n",
    "#print(data.head(10))\n",
    "\n",
    "\n",
    "ALGORITHMS = {0: 'DBSCAN Clustering', 1: 'Mean Shift', 2: 'K-Mediods', 3: 'EM Clustering', 4: 'K-Means'}\n",
    "NUM_ALG = len(ALGORITHMS)\n",
    "original_features = data[features].copy(deep = True)\n",
    "print(original_features.head(10))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "original_features_scaled = scaler.fit_transform(original_features)\n",
    " \n",
    "data['uid'] = data.index # comment this line out for IoT-23\n",
    "uid = \"uid\"\n",
    "ips = data[uid]\n",
    "#print(original_features.columns)\n",
    "#print(ips.head(10))\n",
    "\n",
    "def algorithm_prep(state, action, mode):\n",
    "  selected_features = bin_to_features(state, 0)\n",
    "  out = None\n",
    "  match action:   \n",
    "    case 0:\n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = dbscan_clustering(selected_features, mode)\n",
    "    case 1: \n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = modified_meanshift_clustering(selected_features, mode)\n",
    "    case 2:\n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = modified_kmedoids_clustering(selected_features, mode)\n",
    "    case 3: \n",
    "      out = em_clustering(selected_features, mode)\n",
    "    case 4:\n",
    "      out = kmeans_clustering(selected_features, mode)\n",
    "      \n",
    "  # return silhouette from algorithm function\n",
    "  return out\n",
    "\n",
    "def bin_to_features(state, mode):\n",
    "  state_bin = bin(state)\n",
    "  #print(state_bin)\n",
    "  state_bin_arr = np.array([b for b in state_bin[2:]])\n",
    "  #pad with zeros\n",
    "  diff = len(FEATURES) - len(state_bin_arr)\n",
    "  padded_arr = np.insert(state_bin_arr, 0, ['0' for i in range(diff)])\n",
    "  (padded_arr)\n",
    "\n",
    "  # identify which indexes are 1\n",
    "  idx = (np.where(padded_arr == '1')[0]).tolist()\n",
    "  #print(idx)\n",
    "  # select feature headings\n",
    "  selected_features = original_features.iloc[:,idx]\n",
    "  features = selected_features.columns.tolist()\n",
    "  res = f\"Features Used: {features}\"\n",
    "  if mode == 0: # return actual feature list\n",
    "    return selected_features\n",
    "  if mode == 1: # return string of feature list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045098f",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beedbfe-7d34-401e-b99f-e48e93937a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 1\n",
      "Algorithm: EM Clustering\n",
      "Iteration: 2\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 3\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 4\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 5\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 6\n",
      "Algorithm: K-Means\n",
      "Iteration: 7\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 8\n",
      "Algorithm: K-Mediods\n",
      "Iteration: 9\n",
      "Algorithm: K-Mediods\n",
      "Iteration: 10\n",
      "Algorithm: Mean Shift\n",
      "Iteration: 11\n",
      "Algorithm: K-Means\n",
      "Iteration: 12\n",
      "Algorithm: EM Clustering\n",
      "Iteration: 13\n",
      "Algorithm: EM Clustering\n",
      "Iteration: 14\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 15\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 16\n",
      "Algorithm: Mean Shift\n",
      "Iteration: 17\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 18\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 19\n",
      "Algorithm: Mean Shift\n",
      "Iteration: 20\n",
      "Algorithm: K-Means\n",
      "Iteration: 21\n",
      "Algorithm: K-Mediods\n",
      "Iteration: 22\n",
      "Algorithm: K-Means\n",
      "Iteration: 23\n",
      "Algorithm: K-Means\n",
      "Iteration: 24\n",
      "Algorithm: DBSCAN Clustering\n",
      "Iteration: 25\n",
      "Algorithm: EM Clustering\n",
      "Iteration: 26\n",
      "Algorithm: EM Clustering\n",
      "Iteration: 27\n",
      "Algorithm: EM Clustering\n",
      "Iteration: 28\n",
      "Algorithm: EM Clustering\n",
      "Iteration: 29\n",
      "Algorithm: K-Means\n"
     ]
    }
   ],
   "source": [
    "# Markov Decision Process (MDP) - The Bellman equations adapted to\n",
    "# Q Learning.Reinforcement Learning with the Q action-value(reward) function.\n",
    "# Copyright 2018 Denis Rothman MIT License. See LICENSE.\n",
    "import numpy as ql\n",
    "\n",
    "# R is The Reward Matrix for each state\n",
    "# 1024 configurations of the 10 features --> 2^10\n",
    "# 5 algorithms\n",
    "num_configs = 2 ** len(FEATURES)\n",
    "R = ql.matrix(ql.zeros([num_configs,NUM_ALG]))\n",
    "\n",
    "# Q is the Learning Matrix in which rewards will be learned/stored\n",
    "Q = ql.matrix(ql.zeros([num_configs,NUM_ALG]))\n",
    "\n",
    "# used to save the labels of each (state, action) combination for later retrieval\n",
    "cluster_labels_matrix = np.empty(Q.shape, dtype=object)\n",
    "\n",
    "# Gamma : It's a form of penalty or uncertainty for learning\n",
    "# If the value is 1 , the rewards would be too high.\n",
    "# This way the system knows it is learning.\n",
    "gamma = 0.8\n",
    "\n",
    "# The possible \"a\" actions when the agent is in a given state\n",
    "def possible_actions(state):\n",
    "    # 2) DONE: we should check Q, not R because R is never modified\n",
    "    current_state_row = Q[state,]\n",
    "    # 3) DONE: this should pick valid actions based on what we have not visited\n",
    "    possible_act = ql.where(current_state_row == 0)[1]\n",
    "    return possible_act\n",
    "\n",
    "\n",
    "# This function chooses at random which action to be performed within the range \n",
    "# of all the available actions.\n",
    "\n",
    "def ActionChoice(available_actions_range, state):\n",
    "    epsilon = 0.95 # 90% exploration\n",
    "    if len(available_actions_range) > 0:\n",
    "        if np.random.rand() < epsilon:  \n",
    "            # Explore: Randomly pick from possible actions\n",
    "            next_action = int(ql.random.choice(available_actions_range, 1)[0])\n",
    "        else:\n",
    "            # Exploit: Pick best action from Q matrix\n",
    "            next_action = int(np.argmax(Q[state, :]))\n",
    "    else:\n",
    "    # If no valid actions, pick randomly from all possible algorithms\n",
    "        next_action = int(np.random.choice(NUM_ALG, 1)[0])\n",
    "    \n",
    "    return next_action\n",
    "    \n",
    "\n",
    "\n",
    "# A version of Bellman's equation for reinforcement learning using the Q function\n",
    "# This reinforcement algorithm is a memoryless process\n",
    "# The transition function T from one state to another\n",
    "# is not in the equation below.  T is done by the random choice above\n",
    "\n",
    "def reward(current_state, action, gamma):\n",
    "    Max_State = ql.where(Q[action,] == ql.max(Q[action,]))[1]\n",
    "\n",
    "    if Max_State.shape[0] > 1:\n",
    "        Max_State = int(ql.random.choice(Max_State, size = 1)[0])\n",
    "    else:\n",
    "        Max_State = int(Max_State[0])\n",
    "\n",
    "    MaxValue = Q[Max_State, action]\n",
    "\n",
    "    # call function to run ML algorithm using the value of action. this will\n",
    "    # run the algorithm using the features from current_state, create clusters,\n",
    "    # and calculate the silhouette value.\n",
    "    selected_silhouette_co, labels = algorithm_prep(current_state, action, 0)\n",
    "    cluster_labels_matrix[current_state, action] = labels\n",
    "    try:\n",
    "        overall_silhouette_co = silhouette_score(original_features_scaled, labels)\n",
    "    except ValueError: overall_silhouette_co = -1\n",
    "\n",
    "    # calculate ratio of selected features \n",
    "    ratio = selected_silhouette_co / (overall_silhouette_co + 1e-6)\n",
    "    if selected_silhouette_co < overall_silhouette_co:\n",
    "        penalty = 0.1 * ratio\n",
    "    else: penalty = 0\n",
    "    \n",
    "    # Bellman's MDP based Q function\n",
    "\n",
    "    # normalized silhouette score for better consistency in reinforcement learning\n",
    "    if selected_silhouette_co < 0: # ensures RL doesn't learn from bad clustering\n",
    "        norm_silhouette = 0\n",
    "    else:\n",
    "        norm_silhouette = (selected_silhouette_co + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "    # norm_silhouette = (selected_silhouette_co + 1) / 2\n",
    "    \n",
    "    #norm_silhouette = (silhouette_co + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "    # Q[current_state, action] = norm_silhouette + gamma * MaxValue\n",
    "    Q[current_state, action] = (norm_silhouette - penalty) + gamma * MaxValue\n",
    "\n",
    "\n",
    "# Learning over n iterations depending on the convergence of the system\n",
    "# A convergence function can replace the systematic repeating of the process\n",
    "# by comparing the sum of the Q matrix to that of Q matrix n-1 in the\n",
    "# previous episode\n",
    "\n",
    "# agent_s_state. The agent the name of the system calculating\n",
    "# s is the state the agent is going from and s' the state it's going to\n",
    "# this state can be random or it can be chosen as long as the rest of the choices\n",
    "# are not determined. Randomness is part of this stochastic process\n",
    "# 1) DONE: decide if starting state is random or a specific state\n",
    "#agent_s_state = 1\n",
    "\n",
    "# Get available actions in the current state\n",
    "#PossibleAction = possible_actions(agent_s_state)\n",
    "\n",
    "# Sample next action to be performed\n",
    "#action = ActionChoice(PossibleAction, agent_s_state)\n",
    "\n",
    "# Rewarding Q matrix\n",
    "#reward(agent_s_state,action,gamma)\n",
    "\n",
    "\n",
    "#state_epsilon = 0.95 # 5% exploration\n",
    "visited_pairs = np.zeros(Q.shape, dtype=bool)\n",
    "for a in range(NUM_ALG):\n",
    "    visited_pairs[0,a] = True # to skip all null feature configs\n",
    "\n",
    "convergence_threshold = 0.01  \n",
    "previous_Q = Q.copy()\n",
    "iteration_buffer = num_configs * NUM_ALG\n",
    "\n",
    "for i in range(10000):\n",
    "    print(\"Iteration:\", i)\n",
    "    \n",
    "    # visit all states first, then allow full access to any state\n",
    "    unvisited_pairs = np.argwhere(visited_pairs == False)\n",
    "    state_epsilon = max(0.1, 0.95 * (0.99 ** i)) # starts at 5% exploration/95% exploitation. exploration increases over time but is capped at 90%. \n",
    "\n",
    "    if len(unvisited_pairs) > 0 and ql.random.rand() < state_epsilon:\n",
    "        current_pair = unvisited_pairs[ql.random.choice(len(unvisited_pairs))]\n",
    "        current_state, action = current_pair\n",
    "    else:\n",
    "        if len(unvisited_pairs) == 0:\n",
    "            print('all pairs visited')\n",
    "        if ql.random.rand() < state_epsilon: # explore\n",
    "            current_state = ql.random.randint(1, int(Q.shape[0]))\n",
    "        else: # exploit past good states\n",
    "            k = 10\n",
    "            top_k_states = np.argsort(np.array(Q.sum(axis=1)).flatten())[-k:]\n",
    "            current_state = np.random.choice(top_k_states)\n",
    "        PossibleAction = possible_actions(current_state)\n",
    "        action = ActionChoice(PossibleAction, current_state)\n",
    "    visited_pairs[current_state, action] = True  \n",
    "    \n",
    "    print(\"Algorithm:\", ALGORITHMS[action])\n",
    "    reward(current_state,action,gamma)\n",
    "    #visited_states.add((current_state, action))\n",
    "\n",
    "    if i > iteration_buffer: # make sure it doesn't stop too early\n",
    "    # check for convergence in Q to stop updates\n",
    "        Q_diff = np.abs(Q - previous_Q).sum()\n",
    "        if Q_diff < convergence_threshold:\n",
    "            print(f\"Converged at iteration {i} with Q_diff={Q_diff:.4f}\")\n",
    "            break\n",
    "\n",
    "    previous_Q = Q.copy() # update for comparison\n",
    "    # 95% of the time, we choose the random action and state \n",
    "    \n",
    "# Displaying Q before the norm of Q phase\n",
    "print(\"Q:\")\n",
    "print(Q)\n",
    "\n",
    "# Norm of Q\n",
    "print(\"Normed Q:\")\n",
    "print(Q/ql.max(Q)*100)\n",
    "\n",
    "# DONE: get maximum value from Q-Learning Matrix\n",
    "normed_Q = Q/ql.max(Q)*100\n",
    "max_location = np.where(normed_Q==normed_Q.max())\n",
    "print(\"\\nmax value located at\",max_location)\n",
    "max_config = max_location[0][0]\n",
    "max_algorithm = max_location[1][0]\n",
    "final_feats = bin_to_features(max_config, 1)\n",
    "print(f\"\\nUsing algorithm {ALGORITHMS[max_algorithm]} and {final_feats}, max value is:\",normed_Q[max_config,max_algorithm])\n",
    "#DONE: print(f\"Selected features:\")\n",
    "\n",
    "# get final cluster labels\n",
    "cluster_labels = cluster_labels_matrix[max_config, max_algorithm]\n",
    "\n",
    "# match data to their clusters\n",
    "labelled_data = data.copy()\n",
    "labelled_data['cluster'] = cluster_labels\n",
    "\n",
    "# get total number of clusters\n",
    "num_clusters = labelled_data['cluster'].nunique()\n",
    "\n",
    "### filter clusters based on percentage of data\n",
    "\n",
    "# # for each unique value, get the count / len of data (aka percentage)\n",
    "# cluster_array = labelled_data['cluster'].to_numpy()\n",
    "# perc_values = np.unique(cluster_array,return_counts = True)[-1]\n",
    "# percentages = perc_values / labelled_data.shape[0]\n",
    "\n",
    "# # keep cluster values with % < 10 as anomalous\n",
    "# idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "# anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "\n",
    "\n",
    "### filter anomalous clusters by size relative to the data statistics\n",
    "cluster_sizes = labelled_data['cluster'].value_counts(normalize=True)\n",
    "mean_size = cluster_sizes.mean()\n",
    "std_dev = cluster_sizes.std()\n",
    "flag_val = mean_size\n",
    "anomalous_clusters = cluster_sizes[cluster_sizes < flag_val].index\n",
    "anomalies = labelled_data[labelled_data['cluster'].isin(anomalous_clusters)]\n",
    "\n",
    "# if none fall below the threshold, check if smallest two are statistically different\n",
    "if len(anomalous_clusters) == 0:\n",
    "    sorted_clusters = cluster_sizes.sort_values()\n",
    "    sm, sec_sm = sorted_clusters.iloc[0], sorted_clusters.iloc[1]\n",
    "\n",
    "    if sm < (0.8 * sec_sm):  # sm is at least 20% smaller than sec_sm\n",
    "        anomalies = labelled_data[labelled_data['cluster'] == sorted_clusters.index[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f631c887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters\n",
    "\n",
    "a = labelled_data.loc[labelled_data['cluster'] == 2]\n",
    "# set(a['Label'])\n",
    "len(a['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b57472d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using algorithm DBSCAN Clustering and Features Used: ['Djit', 'Dload', 'ct_dst_sport_ltm', 'ct_srv_dst', 'ct_state_ttl', 'dmeansz'], max value is: 100.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcip</th>\n",
       "      <th>sport</th>\n",
       "      <th>dstip</th>\n",
       "      <th>dsport</th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>Label</th>\n",
       "      <th>uid</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.40.182.1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50.004372</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59.166.0.3</td>\n",
       "      <td>26273</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.436850</td>\n",
       "      <td>12472</td>\n",
       "      <td>12716</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>59.166.0.2</td>\n",
       "      <td>64156</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>59.166.0.1</td>\n",
       "      <td>46078</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.881681</td>\n",
       "      <td>2934</td>\n",
       "      <td>3740</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>59.166.0.7</td>\n",
       "      <td>18544</td>\n",
       "      <td>9</td>\n",
       "      <td>6881</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.656051</td>\n",
       "      <td>23202</td>\n",
       "      <td>1052048</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>59.166.0.1</td>\n",
       "      <td>25295</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1934</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>59.166.0.8</td>\n",
       "      <td>1764</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.203732</td>\n",
       "      <td>12752</td>\n",
       "      <td>13202</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1946</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>59.166.0.4</td>\n",
       "      <td>20206</td>\n",
       "      <td>5</td>\n",
       "      <td>49992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.860602</td>\n",
       "      <td>320</td>\n",
       "      <td>1878</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>42272</td>\n",
       "      <td>19</td>\n",
       "      <td>520</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1144</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1988</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>59.166.0.4</td>\n",
       "      <td>58735</td>\n",
       "      <td>2</td>\n",
       "      <td>6881</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.848744</td>\n",
       "      <td>24848</td>\n",
       "      <td>1094788</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             srcip  sport  dstip  dsport  proto  state        dur  sbytes  \\\n",
       "11     10.40.182.1      0      7       0      2      2  50.004372     384   \n",
       "20      59.166.0.3  26273      5      22      1      1   1.436850   12472   \n",
       "23      59.166.0.2  64156      6      53      0      0   0.000990     146   \n",
       "30      59.166.0.1  46078      0      21      1      1   1.881681    2934   \n",
       "38      59.166.0.7  18544      9    6881      1      1   5.656051   23202   \n",
       "...            ...    ...    ...     ...    ...    ...        ...     ...   \n",
       "1934    59.166.0.1  25295      8      53      0      0   0.001008     146   \n",
       "1946    59.166.0.8   1764     11      22      1      1   0.203732   12752   \n",
       "1969    59.166.0.4  20206      5   49992      1      1   0.860602     320   \n",
       "1988  175.45.176.0  42272     19     520      0      3   0.000006    1144   \n",
       "1996    59.166.0.4  58735      2    6881      1      1   5.848744   24848   \n",
       "\n",
       "       dbytes  sttl  ...  ct_srv_src  ct_srv_dst  ct_dst_ltm  ct_src_ ltm  \\\n",
       "11          0     1  ...           2           4           4            2   \n",
       "20      12716    31  ...          10           7           6           10   \n",
       "23        178    31  ...           3           3          11            9   \n",
       "30       3740    31  ...           4           4          17           30   \n",
       "38    1052048    31  ...           8          13           4            1   \n",
       "...       ...   ...  ...         ...         ...         ...          ...   \n",
       "1934      178    31  ...           2           2           5           13   \n",
       "1946    13202    31  ...           9          11           8            4   \n",
       "1969     1878    31  ...          23           7           8           18   \n",
       "1988        0   254  ...           5           5           3            2   \n",
       "1996  1094788    31  ...          10          12           4            7   \n",
       "\n",
       "      ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  Label   uid  cluster  \n",
       "11                   2                 4               2      0    11        2  \n",
       "20                   4                 4               4      0    20        2  \n",
       "23                   3                 3               6      0    23        3  \n",
       "30                   4                 4               9      0    30        2  \n",
       "38                   1                 1               1      0    38        4  \n",
       "...                ...               ...             ...    ...   ...      ...  \n",
       "1934                 2                 2               2      0  1934        6  \n",
       "1946                 4                 4               4      0  1946        2  \n",
       "1969                 3                 3               8      0  1969        3  \n",
       "1988                 2                 1               2      1  1988       15  \n",
       "1996                 1                 1               2      0  1996        4  \n",
       "\n",
       "[215 rows x 50 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nUsing algorithm {ALGORITHMS[max_algorithm]} and {final_feats}, max value is:\",normed_Q[max_config,max_algorithm])\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc67922",
   "metadata": {},
   "source": [
    "Using algorithm DBSCAN Clustering and Features Used: ['ct_dst_sport_ltm', 'ct_state_ttl', 'res_bdy_len', 'state'], max value is: 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25122174",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4539ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc32dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      true pred   uid\n",
      "15       1    0    15\n",
      "93       1    0    93\n",
      "129      1    1   129\n",
      "229      1    1   229\n",
      "260      1    0   260\n",
      "...    ...  ...   ...\n",
      "1775     1    0  1775\n",
      "1782     1    1  1782\n",
      "1810     1    1  1810\n",
      "1820     1    1  1820\n",
      "1988     1    1  1988\n",
      "\n",
      "[63 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# NB15 = Label | IOT = label | KDD'99 = Label | NSL-KDD = Class\n",
    "\n",
    "labels = data['Label']\n",
    "result_labels = pd.DataFrame(columns=[\"true\", \"pred\"])\n",
    "result_labels['uid'] = ips\n",
    "result_labels['true'] = labels\n",
    "#print(result_labels)\n",
    "result_labels.loc[result_labels['uid'].isin(anomalies[uid]), 'pred'] = 1\n",
    "result_labels.loc[~result_labels['uid'].isin(anomalies[uid]), 'pred'] = 0\n",
    "print(result_labels.loc[result_labels['true'] == 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78c29dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = result_labels['true'].astype(int)\n",
    "y_pred = result_labels['pred'].astype(int)\n",
    "\n",
    "pr_auc = average_precision_score(y_true, y_pred)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f3f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC: 0.13510483573274273\n",
      "ROC AUC: 0.7804820086699282\n",
      "Accuracy: 0.902\n",
      "F1: 0.2949640287769784\n",
      "Precision: 0.19069767441860466, Recall: 0.6507936507936508\n",
      "TP: 41, TN: 1763, FP: 174, FN: 22\n"
     ]
    }
   ],
   "source": [
    "print(\"PR AUC:\", pr_auc)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1:\", f1)\n",
    "print(f\"Precision: {precision}, Recall: {recall}\")\n",
    "print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
